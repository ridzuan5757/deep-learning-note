\newpage
\section{Chapter 2}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_perceptron}
\end{figure}

\hfill\break
Each individual neurons can be represented as a threshold unit.
\begin{itemize}
	\item It fires if the affine function of inputs is positive.
	\item The bias value is the negative of threshold $T$.
\end{itemize} 

\begin{align*}
	z &= \sum_{i}w_ix_i + b \\
	y &= 
\begin{cases} 
	1, & \text{if } z \geq 0 \\
	0, & \text{else }
\end{cases}
\end{align*}

\hfill\break
Once we can represent the neuron in this manner, we can modify the activation threshold function to something different such as:
\begin{itemize}
	\item We define \textbf{activation function} as the function that acts on the weighted combination of inputs and bias.
	 
\end{itemize}
\paragraph{Soft Perceptron (Logistic)}
A squashing function instead of a threshold at the output. This way the output goes rather smooth from $0$ to $1$.
\begin{align*}
	z &= \sum_{i}w_ix_i + b \\
	y &= \frac{1}{1 + \exp (-z)}
\end{align*}

\hfill\break
We can also replace the activation function with other mathematical function such as $\tanh$, $\text{softplus}$ and $\text{rectifier}$.

\subsection{Multi-layer Perceptron}
MLP is a network of perceptrons as the perceptrons of current layer are fed to other perceptrons on the next layer.

\paragraph{Deep Structures}
In any directed graph with input source nodes and output sink nodes, ``depth'' is the length of the longest path from a source to a sink.
\begin{itemize}
	\item A ``source'' node in a directed graph is a node that has only outgoing edges.
	\item A ``sink'' node is a node that has only incoming edges.
\end{itemize}

\hfill\break
This is an example of depth 2 graph.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{2_2depth}
\end{figure}

\hfill\break
This is an example of depth 3 graph.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\textwidth]{2_3depth}
\end{figure}

\hfill\break
In multilayer perceptron, a network is considered ``deep'' if the depth of the output neurons is greater than $2$.

\subsection{Layer}
Layer is a set of neurons that are all at the same depth  with respect to the input (sink). This would imply that the ``depth'' of the layer is the depth of the neurons in the layer with respect to the input.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{2_layer}
\end{figure}

\hfill\break
In multi-layered perceptron:
\begin{itemize}
	\item Inputs are real or Boolean stimuli.
	\item Outputs are real or Boolean values.
	\item It can compose both Boolean and real-valued functions.
	\item We can have multiple outputs for a single input.
\end{itemize}

\subsection{MLP as universal Boolean functions.}
We are already aware with perceptron capability as a Boolean gate as it can model any simple binary Boolean gate (OR, AND \& NOT).

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{2_boolean_gate}
\end{figure}

\begin{itemize}
	\item For the AND gate, the boundary condition is $X + Y \geq 2$
	\item For the OR gate, the boundary condition is $X + Y \geq 1$
	\item For the NOT gate, the boundary condition is $-X \geq 0$
\end{itemize}

\hfill\break
It also enables us to compose a much more complex network, for example a universal AND gate where the network is only fired if every input fed to the network is true.

\paragraph{Universal AND Gate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_universal_and}
\end{figure}

Suppose that $X_1,X_2,...,X_L$ has weight value of $1$ and $X_{L+1},X_{L+2},...,X_N$ has weight value of $-1$, the condition for the perceptron to fire has to be:
\begin{align}
	(X_1+X_2+...+X_L) - (X_{L+1},X_{L+2},...,X_N) \geq L
\end{align}

\paragraph{Universal OR Gate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_universal_or}
\end{figure}

Suppose that $X_1,X_2,...,X_L$ has weight value of $1$ and $X_{L+1},X_{L+2},...,X_N$ has weight value of $-1$, the condition for the perceptron to fire has to be if any value of the first set is $1$ or any value from the second set is $0$:
\begin{align}
	(X_1+X_2+...+X_L) - (X_{L+1},X_{L+2},...,X_N) \geq L - N + 1
\end{align}

\paragraph{Generalized Majority Gate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_majority_gate}
\end{figure}

\hfill\break
Due to perceptrons being able to model Boolean functions, they able to model complex things that are made of Boolean functions. This also means that for any odd Boolean function, wee can construct a multi-layerr perceptron that can compute this Boolean function. This is what is meant by MLPs being a universal Boolean functions. For any Boolean functions, regardless of their complexity, we can compose an MLP that will compute said function. This lead to a new question, how many layers will they need to be sufficient at modeling said Boolean function? What is the smallest number of layers will be required?


\hfill\break
Any Booleans function can be represented by a truth table. For example:


\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			$X_1$    & $X_2$     & $X_3$    & $X_4$    & $X_5$    & $Y$   \\
			\hline
			0        & 0        & 1        & 1        & 0        & 1     \\
			0        & 1        & 0        & 1        & 1        & 1     \\
			0        & 1        & 1        & 0        & 0        & 1     \\
			1        & 0        & 0        & 0        & 1        & 1     \\
			1        & 0        & 1        & 1        & 1        & 1     \\
			1        & 1        & 0        & 0        & 1        & 1     \\
			\hline
	\end{tabular}
\end{table}

\hfill\break
In order to get a Boolean function that represent this table, we only have to list the portion of the table that corresponds to the output 1. We can write it as disjunctive normal formula for this table. 

\begin{align}
	Y = \bar{X_1}\bar{X_2}X_3X_4\bar{X_5} + \bar{X_1}X_2\bar{X_3}X_4X_5 + \bar{X_1}X_2X_3\bar{X_4}\bar{X_5} +\\ X_1\bar{X_2}\bar{X_3}\bar{X_4}X_5 + X_1\bar{X_2}X_3X_4X_5 + X_1X_2\bar{X_3}\bar{X_4}X_5
\end{align}

\hfill\break
Once we have obtained the Boolean function in this manner, we can create an MLP where each of the perceptron is an Universal AND gate. These perceptrons is then combined as Universal OR gate.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_truth_table}
\end{figure}

\hfill\break
This would means that MLP with 1 layer is sufficient enough to model any Boolean Function. This lead to a new question, what is the largest number of perceptrons required in the single hidden layer for an N-input-variable function?

\subsection{Karnaugh Map}
Boolean function can be reduced using Karnaugh Map. It represent a truth table as a grid. Adjacent boxes can be ``grouped'' to reduce the DNF formula for the table.

%\begin{figure}[H]
%\centering
%\begin{Karnaugh}{$v_w$}{$x_y$}
%	\content{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}
%	\implicant{0}{2}{red}
%	\implicant{5}{15}{purple}
%	\implicanttopbottom[3pt]{1}{10}{blue}
%	\implicantcorners[2pt]{orange}
%	\implicantside{4}{14}{green}
%\end{Karnaugh}
%\end{figure}

\begin{figure}[H]
	\centering
	\begin{Karnaugh}{$WX$}{$YZ$}
		\content{1,0,1,0,1,1,0,0,1,0,1,0,1,0,0,0}
		\implicant{4}{5}{red}
		\implicant{0}{8}{blue}
		\implicanttopbottom[3pt]{2}{10}{green}
	\end{Karnaugh}
\end{figure}

\hfill\break
We can group these 3 neighbours together:

\begin{align}
	O = \bar{Y}\bar{Z} + \bar{W}X\bar{Y} + \bar{X}Y\bar{Z}
\end{align}

\begin{itemize}
	\item Regardless value of $W$ and $X$, $Y$ and $Z$ is always $0$.
	\item The function going to fire when $W$ is always $0$ while $X$ can be anything, while $Y$ has to always be $0$.
	\item The other condition for it to fire is when $Y$ is always $1$ while $Z$ is always $0$ and $X$ is always $0$.
\end{itemize}


\hfill\break
Although there are 8 conditions when the network will fire, only 3 perceptions is needed to construct the MLP. The general idea is, when we want to construct the DNF formula, we want to construct it with thee minimum number of clauses.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_reduced_dnf}
\end{figure}

\hfill\break
On irreducible DNF, the size of required perceptrons in the hidden layer can get exponentially large ($2^{n-1}$).

\begin{figure}[H]
	\centering
	\begin{Karnaugh}{$WX$}{$YZ$}
		\content{1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1}
	\end{Karnaugh}
\end{figure}

\hfill\break
While the DNF cannot be reduced, this type of Boolean function can still be generalized. Is it possible to reduce the number of units if we use multiple hidden layers?

This checkered pattern is a form of XOR Boolean function which become TRUE only if:
\begin{itemize}
	\item 1 flag is FALSE while the rest of the flags is TRUE.
	\item 1 flag is TRUE while the rest of the flags is FALSE.
\end{itemize}

\begin{align}
	O = W \oplus X \oplus Y \oplus Z
\end{align}

\hfill\break
Since we need 1 hidden layer consisting of 2 perceptrons to construct XOR gate:

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_xor}
\end{figure}

\hfill\break
We can cascade the XOR gates to construct the solution for the MLP.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_xor_cascade}
\end{figure}

\hfill\break
For cascading XOR gates with 4 parameter, 9 perceptrons would be required to construct the Boolean function MLP. The solution of the cascading XOR gates would be $3\times (N-1)$ number of perceptrons. 

These can be arranged in only $2\log_2(N)$ layers. Suppose that wee have $X_1, X_2, ..., X_N$ number of perception. We can keep pairing terms such as:

\begin{align}
	O &= X_1 \oplus X_2 \oplus X_3 \oplus X_4 \oplus X_5 \oplus X_6 \oplus X_7 \oplus X_8\\
	O &= ((X_1 \oplus X_2) \oplus (X_3 \oplus X_4)) \oplus ((X_5 \oplus X_6) \oplus (X_7 \oplus X_8))
\end{align}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{2_multi_layer_xor}
\end{figure}

\subsection{Implementation of MLP on K-layer.}
Using only K-hidden layers will require $2^{CN}$ neurons in the K-th layer, where

\begin{align}
	C = 2^{-\frac{K-1}{2}}
\end{align}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{2_k_layer_xor}
\end{figure}

\hfill\break
Going through the layer until the K-th layer, we keep reducing the number of neutrons by factor of $2$. However, we still have to XOR whatever number of neutrons left on the K-th layer, exponentially increasing the number of neutrons.

\begin{itemize}
	\item Because the output is the XOR for all the $\frac{N}{2^{\frac{K-1}{2}}}$ values output by the K-1-th hidden layer.
	\item In other word, reducing the numbers of layers below the minimum will result in an exponentially sized network to express the function fully.
	\item A network with fewer than minimum required number of neutrons cannot model the function.
\end{itemize}

\subsection{Network Parameters.}
The actual number of parameters in a network is the number of connections. For example, for a network with 1 hidden layer consisting of 5 neutrons, the number of parameters in a network is 30.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{2_1_layer_mlp}
\end{figure}

This is the number that really matters in software or hardware implementation of the neural networks. Networks that require an exponential number of neurons will require an exponential number of weights.

\subsection{Composing complicated ``decision'' boundary.}
Once we can computes a linear boundary, we can compose more complex decision boundary as shown below.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{2_penta_boundary}
\end{figure}

\hfill\break
Recall that should we have pentagonal boundary, that would means that we have boundaries for each sides of the pentagon and the network will fire if the input value exceeds the boundary threshold value. One thing to notice that, only within the pentagon are all five perceptrons going to output $1$. This would imply that the network will fire if \textbf{sum of all of the output} exceeds the \textbf{sum of the threshold boundary value}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{2_sum_output}
\end{figure}

And if there is more than one closed boundaries, we have to OR them together to get the complex decision boundary. This would means that a third layer would be required.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{1_complex}
\end{figure}

The question is, is it possible to compose these decision boundaries with only one hidden layer?

\paragraph{Case: Square boundary}
The sum of the output of the inner boundary is 4. Instead of having output layer of AND-ing all of the output of the previous layer perceptron, we just set the threshold value to be the sum of the threshold value of the individual perceptron. The last output neuron is performing SUM instead of AND.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{2_square_boundary}
\end{figure}

\paragraph{Case: Pentagon boundary}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{2_pentagon_boundary}
\end{figure}

\paragraph{Case: Hexagon boundary}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{2_hexagon_boundary}
\end{figure}














